Working through data grooming pipeline

# first step produce fastqc files
fastqc -o output/ tinytestR1.fastq tinytestR2.fastq

#1. Initial quality control (fastqc)
  INPUT: Raw fastq files
  command: fastqc -o output/ libR1.fastq.gz libR2.fastq.gz
  OUTPUT: untouched raw fastq file and fastqc zip
  
#2. Quality and/or length-based reads discarding; trimming/discarding of N-containing reads (Trimmomatic),
Have to crop the first three bases off the front FIRST, with R1 reads
  INPUT: $libR1 fastq file
  Command: java -jar trimmomatic/trimmomatic.jar SE -phred33 -trimlog headcrop.trimlog $READSDIR/libR1.fastq.gz $READSDIR/libR1crped.fastq.gz HEADCROP:5
  OUTPUT: $libR1crped.fastq.gz
  
We have a very large number (not sure how many) of 35 bp reads that contain only N. Their quality score is really low, too. 
But in order to get a good sense of this, it would be nice to do one quick pass with something to just dash all of those to a separate file?
We could just do a length trim first, minlen:36
  INPUT: $libR1crped.fastq.gz $libr2.fastq.gz
  command: java -jar trimmomatic/trimmomatic.jar PE -phred33 -trimlog lib.trimlog $READSDIR/$libR1crped.fastq.gz $READSDIR/$libR2.fastq.gz Output/$libP1.step2.fastq.gz Output/$libU1.step2.fastq.gz Output/$libP2.step2.fastq.gz Output/$libU2.step2.fastq.gz MINLEN:36
  OUTPUT: $libP1.step2.fastq.gz $libU1.step2.fastq.gz $libP2.step2.fastq.gz $libU2.step2.fastq.gz
	  Report from Trimmomatic about how many reads got ditched --> that output needs to be sent to a file
	  
#3. Adapter removal (Trimmomatic with above step)
  Trimmomatic with IlluminaClip. We should also at this point just do the full trimmomatic culling that we do: 
This is done in two steps -- one PE run and one SE run for both unpaired files (if they exist -- hopefully the short read culling doesn't create unpaired files)
 INPUT: $libP1.step2.fastq.gz $libP2.step2.fastq.gz
 command: java -jar trimmomatic/trimmomatic.jar PE -phred33 -trimlog $lib.trimlog $READSDIR/$libP1.step2.fastq.gz $READSDIR/$libP2.step2.fastq.gz Output/$libP1.step3.fastq.gz Output/$libU1.step3.fastq.gz Output/$libP2.step3.fastq.gz Output/$libU2.step3.fastq.gz ILLUMINACLIP:adapters1.fasta:2:25:10:1:true LEADING:10 TRAILING:10 SLIDINGWINDOW:4:20 CROP:222  MINLEN:75
 
 ##Possibly an unnecessary step depending on output of step2 -- maybe no unpaired reads from step2
 INPUT: $libU1.step2.fastq.gz $libU2.step2.fastq.gz
 command: java -jar trimmomatic/trimmomatic.jar SE -phred33 -trimlog $lib.trimlog $READSDIR/$libU1.step2.fastq.gz Output/$libU1.step3a.fastq.gz ILLUMINACLIP:adapters1.fasta:2:25:10:1:true LEADING:10 TRAILING:10 SLIDINGWINDOW:4:20 CROP:222  MINLEN:75
 java -jar trimmomatic/trimmomatic.jar SE -phred33 -trimlog $lib.trimlog $READSDIR/$libU2.step2.fastq.gz Output/$libU2.step3a.fastq.gz ILLUMINACLIP:adapters1.fasta:2:25:10:1:true LEADING:10 TRAILING:10 SLIDINGWINDOW:4:20 CROP:222  MINLEN:75
 OUTPUT: $libP1.step3.fastq.gz $libP2.step3.fastq.gz $libU1.step3.fastq.gz $libU2.step3.fastq.gz $libU1.step3a.fastq.gz $libU2.step3a.fastq.gz
 
 ##Possibly unnecessary
 cleanup step: At this point let's put the unpaired reads together again...
 gunzip *U*step3*.fastq.gz 
 cat $libU1.step3a.fastq >> $libU1.step3.fastq
 cat $libU2.step3a.fastq >> $libU2.step3.fastq 
 
  
 
#4. phix/contaminant removal
bowtie on -human genome -saccharomyces -ecoli -phix

INPUT: bowtie indices for hg, sach, ecoli
	Filtered fastq files
	
	command: 
	bowtie -ft --refout --al $libP1.contaminants.fq --un $libP1_filtered.fq /apps/bowtie/1.0.0/indexes/hg19 $libP1.step3.fastq 
	bowtie2 -q --phred33 --mm --very-fast -k 1 -I 250 -X 1000 --dovetail --met-file bowtie2Metrics-rRNA.out --un $libUnpaired%.filtered.fastq --al $libUnpaired%.contams.fastq --un-conc $libP%.filtered.fastq --al-conc $libP%.contams.fastq -x $indexPath -1 $libP1.step3.fastq -2 $libP2.step3.fastq -S $libpaired$index.sam

	This will need to be a function that accepts the lib prefix, the index, and the path to the index. 
	
	Then I can call the function for each index. 
	

#7. Merging of PE reads (SeqPrep?)
INPUT: final filtered reads - $lib.P1.filtered.fastq $lib.P2.filtered.fastq
#8. Quality trimming + phix/contaminant removal
#9. Contamination check


==== June 20 2015

Writing the SeqPrep functions

INPUT: $libP%.filtered.f
command: SeqPrep -f $lib.P1.filtered.fastq -r $lib.P2.filtered.fastq -1 $lib.P1.SP.fastq -2 $lib.P2.SP.fastq -s $lib.merged.SP.fastq -g -E $lib.alignments.fasta -A -B 

OUTPUT: Three new files -- <lib-base-name>.P1/2.SP.fastq -- the final files from SeqPrep. These have been merged if possible. 

Complete the data grooming script v.1, and tested on real data. The script made it through the trimmomatic steps, but we need to do some work on the bowtie steps. 
Propose going to another script at this point, that we pass in the lib base name and a list of bt2 index base names. The script can move through the bt2 indices, renaming the filtered 
reads each time. 

pseudocode
set $lib-base-name to something sensible that will get picked up on the first pass
for each index in index list
 
 run bowtie2 in PE mode on the paired reads for lib-base-name
      (make sure the name of the filtered reads is set to something that includes that index)
 
 run bowtie2 in SE mode on the single end reads for lib-base-name
      (make sure the name of the filtered reads is set to something that includes that index)
      
 change the variable $lib to be lib-base-name + index.filtered (so that it will find the now filtered reads on the next pass
 next
 
===

How to run bowtie2 more effectively:
Set the --no-mixed option -- this means we won't find unpaired alignments, which we're okay with. 

FTFM: 
If Bowtie 2 cannot find a paired-end alignment for a pair, by default it will go on to look for unpaired alignments for the constituent mates. This is called "mixed mode." To disable mixed mode, set the --no-mixed option.

Set --dovetail because we do definitely want to be able to keep reads that overlap completely -- we will need to be able to check them for adapter readthrough

On the server, we can use the -p option to use multiple threads in parallel. rather than the -mm?
On the server, use --sensitive rather than --very-fast because we want bowtie2 to be a bit more sure that this is the right place to align the reads.

do NOT set -k 1 because we want it to go through a bit more effort to find the right place to map the pairs. 
We don't want it to just report the first one it finds. 

So:

bowtie2 -q --phred33 -p 4 --no-mixed --sensitive -I 250 -X 800 --dovetail --met-file bowtie2Metrics-$index.out --un $libbase$index.Unpaired.filtered.fastq --al $libbase$index.Unpaired.contams.fastq --un-conc $libbase$index.P%.filtered.fastq --al-conc $libbase$index.P%.contams.fastq -x $indexPath/$index -1 $pass.P1.filtered.fastq -2 $pass.P2.filtered.fastq -S $pass.paired.$index.sam

=== June 23 2015
OK, I've finally got this working pretty well! The only thing I need to do is hook it up to the Trimmomatic steps. Let's go back to our tiny test!

Made tiny test (40000 lines) of 1387. Have put it through the pipeline up to the step of filtering contams from the 
unpaired (single end) reads. This is all working pretty much splendidly. 

How shall I write this part? We have a nice loop going for the paired end:
  for i in $indices
  do
    echo $i
    echo "----"
    filterContamsPE $lib $i $indexPath 
    echo "-----"
    echo $lib 
  done

  And the filterContamsPE function is able to find the reads that were filtered from the last time because at the end of the filtering it renames them to what it expected to begin with. 
  
  With the filterContamsSE function, we SHOULD be able to do much the same thing. The reads will get a new name from bowtie, $lib.$index.U.filtered.fastq and $lib.index.U.contams.fastq -- and after we finish bowtie, we should rename the "filtered" one to be what we expected it to be to begin with. 
  
  One thing we have to deal with is that the filterContamsPE will be creating small sets of unpairedfiltered reads as it goes. We have to make sure that those all get collected and don't fall by the wayside. And then, the maximally efficient thing to do is to cat all those together 
  
==== 6/26/2015

I HAVE MY DATA BACK FROM MY KAPA RUNS!
There is some adapter/primer contamination. Running my grep/substrings perl script on library 4-ECbPro:

(This is for PCRPrimer1 which is also PrefixPE1/1)
TCTACACTCTTTCCCTACAC: 36
 
CTACACTCTTTCCCTACACG: 34

TACACTCTTTCCCTACACGA: 34
 
ACACTCTTTCCCTACACGAC: 35
 
CACTCTTTCCCTACACGACG: 37
 
ACTCTTTCCCTACACGACGC: 35
 
CTCTTTCCCTACACGACGCT: 34
 
TCTTTCCCTACACGACGCTC: 34
 
CTTTCCCTACACGACGCTCT: 31

Nothing like the flowcell contams we have with the Clontech libraries, but this is still pretty important to get rid of!! That's definitely enough contamination to seriously screw up 
assemblies AND FPKM counts. 

We should definitely make sure that the IlluminaClip takes care of this. 

==== 6/27/15

OK, so we definitely have PRCprimer 1 contamination in the KAPA libraries. Want to run Illumina Clip on them to make damn sure we don't include that
in our assemblies!!

But we really need to know if Illumina Clip is doing the job we think it's doing -- if not, we need to use CutAdapt or even just drop these by running a perl script that just
ditches the whole read if we find contams. I'm finding a max of like 10,000 reads in the R2 reads, which is the tiniest drop in the bucket relative to my data -- but will REALLY screw up 
the assembly. 

The sequence I'm finding in R2 is:
AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT <-- PRC primer 1_rc
  And a little bit of flow-cell:
GTGTAGATCTCGGTGGTCGCCGTATCATTAAAAAAAAAA <-- flowcell adapter rc
  
  java -jar /home/cfisher/bio/apps/trimmomatic/trimmomatic.jar PE -phred33 -trimlog 4ECproTrim.log 4-ECbPro.R1.fastq 4-ECbPro.R2.fastq scratch/4-ECbPro.P1.trimmed.fastq scratch/4-ECbPro.U1.trimmed.fast scratch/4-ECbPro.P2.trimmed.fastq scratch/4-ECbPro.U2.trimmed.fastq ILLUMINACLIP:/home/cfisher/bio/apps/trimmomatic/adapters/Illumina.fa:2:30:10
ch/4-ECbPro.P1.trimmed.fastq scratch/4-ECbPro.U1.trimmed.fast scratch/4-ECbPro.P2.trimmed.fastq scratch/4-ECbPro.U2.trimmed.fastq ILLUMINACLIP:/home/cfisher/bio/apps/trimmomatic/adapters/Illumina.fa:2:30:10
Using PrefixPair: 'AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'CAAGCAGAAGACGGCATACGAGATCGGTCTCGGCATTCCTGCTGAACCGCTCTTCCGATCT'
Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'
Using Long Clipping Sequence: 'TTTTTTTTTTAATGATACGGCGACCACCGAGATCTACAC'
Using Long Clipping Sequence: 'TTTTTTTTTTCAAGCAGAAGACGGCATACGA'
Using Long Clipping Sequence: 'AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT'
Using Long Clipping Sequence: 'GTGTAGATCTCGGTGGTCGCCGTATCATTAAAAAAAAAA'
Using Long Clipping Sequence: 'AGATCGGAAGAGCGGTTCAGCAGGAATGCCGAGACCGATCTCGTATGCCGTCTTCTGCTTG'
Using Long Clipping Sequence: 'CAAGCAGAAGACGGCATACGAGATCGGTCTCGGCATTCCTGCTGAACCGCTCTTCCGATCT'
Using Long Clipping Sequence: 'AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT'
ILLUMINACLIP: Using 2 prefix pairs, 7 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences

RESULT: 
Input Read Pairs: 5443843 Both Surviving: 5400270 (99.20%) Forward Only Surviving: 43541 (0.80%) Reverse Only Surviving: 14 (0.00%) Dropped: 18 (0.00%)


This very helpful website explains that if we have PCR primer 1 contamination on our R2 reads, then we should have corresponding PCR primer 2 contamination on our R1 reads:
https://wikis.utexas.edu/display/bioiteam/Evaluating+your+raw+sequencing+data

AND, doing the getsubstrings routine on R1 reads with the PCR 2 primer DOES in fact reveal:
4-ECbPro.R1.fastq 
AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC
AGATCGGAAGAGCACACGTC: 9726
 
TCGGAAGAGCACACGTCTGA: 10805
 
GAAGAGCACACGTCTGAACT: 10034
 
GAGCACACGTCTGAACTCCA: 9393
 
CACACGTCTGAACTCCAGTC: 7993

Which is about what we saw in R2.

The utexas page recommends using cutadapt for this.
Which IS installed on the cluster!

Let's see how cutadapt does on the same files, using the following sequences--
R2 primer (contaminating R1 reads)
AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC
AGATCGGAAGAGCGGTTCAGCAGGAATGCCGAGACCGATCTCGTATGCCGTCTTCTGCTTG
                            


and R1 primer (contaminating R2 reads)
AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTA
AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT

so this will be
cutadapt -m 35 -O 10 -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC -o 4-ECbPro.R1.cutadapt.fastq
(for R1 reads)
and cutadapt -m 35 -O 10 -a AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTA -o 4-ECbPro.R2.cutadapt.fastq
(for R2 reads)


R1 reads:
Command line parameters: -m 35 -O 10 -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC 4-ECbPro.R1.fastq
Maximum error rate: 10.00%
   No. of adapters: 1
   Processed reads:      5443843
   Processed bases:    544384300 bp (544.4 Mbp)
     Trimmed reads:        36085 (0.7%)
     Trimmed bases:      1102130 bp (1.1 Mbp) (0.20% of total)
   Too short reads:         5978 (0.1% of processed reads)
    Too long reads:            0 (0.0% of processed reads)
        Total time:    208.81 s
     Time per read:      0.04 ms
=== Adapter 1 ===
Adapter 'AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC', length 34, was trimmed 36085 times.
No. of allowed errors:
0-9 bp: 0; 10-19 bp: 1; 20-29 bp: 2; 30-34 bp: 3


switching forks -- trying to get ths datagroom pipeline just running by itself on the cluster -- it's now ready to go for both salamanders and bugs.
But when I try to qsub -- the script can't run because it moves to a node where it doesn't have all of the files it wants. So I need to be able to get it to use
the files that are in my home directory. :-/ 

NO, really having a LOT of trouble with this. 

OK, right now all I want to do is quickly run fastqcBefore and then Trimmomatic on the ECc group of reads. 
I've updated datagroombugs.sh to have all the functions in it. It SHOULD WORK if I can move all of the files to the right directory! 

That worked for trimmomatic stuff. I submitted a job called runtrim.sh that contained just the command:
bash datagroombugs.sh $lib $TRIMPATH bt2

job was submitted like so:
qsub -o <libbase>.out -e <libbase>.err -N Trim<libbase> runtrim.sh <libbase>

Ran fastqc on the output...
So the
quality looks a lot better, but it does not seem to have actually gotten all of the adapter sequence. There are still overrepresented sequences that look like primer sequences. 

SO! 

We have to run Bowtie next, because Bowtie will fail PE if the number of reads isn't the same, and cutadapt will ditch reads indiscriminately (doesn't have a PE mode)
Running bowtie on... 
hg19 (for humans!) and e_coli (bacteria!)
it'd be nice if I had an RNA index, but I don't -- I'll have to make one from the RNA contig in my first assembly. 

oh man bowtie is sooooooo slow

I have to figure out how I'm going to do this without babysitting it the whole time. I really SHOULD be able to just run all the scripts at once.

I suppose I could write a quick script that did all the QSubs, too. 

Back on the vertebrates side of things, ELJ's clontech libraries DEFINITELY have readthrough, but the Nextera libraries do NOT. This makes a lot of sense, since the Nextera libraries were WAY longer than
the clontech ones are. A useful thing to think about -- maybe it really is best to size select out the little stuff as much as possible. 
As a comparison, here are the counts for contams in the ELJ1387 R2 reads
ra@rockapella:~/DATA/ClonTechLibs$ perl ~/scripts/utilities/getsubstrings.pl $SEARCH ELJ1387_S6_L001_R2_001.fastq 
AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT
AGATCGGAAGAGCGTCGTGT: 3030
TCGGAAGAGCGTCGTGTAGG: 2909
GAAGAGCGTCGTGTAGGGAA: 2479
GAGCGTCGTGTAGGGAAAGA: 2372
CGTCGTGTAGGGAAAGAGTG: 3621
CGTGTAGGGAAAGAGTGTAG: 4120
GTAGGGAAAGAGTGTAGATC: 5132
GGGAAAGAGTGTAGATCTCG: 6087
AAAGAGTGTAGATCTCGGTG: 6609
GAGTGTAGATCTCGGTGGTC: 6850
TGTAGATCTCGGTGGTCGCC: 6958
AGATCTCGGTGGTCGCCGTA: 6653 
TCTCGGTGGTCGCCGTATCA: 6200

versus the ELJ1393 R2 reads, which we size selected

cera@rockapella:~/DATA/ClonTechLibs$ perl ~/scripts/utilities/getsubstrings.pl $SEARCH ELJ1393_S7_L001_R2_001.fastq 
AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT
AGATCGGAAGAGCGTCGTGT: 165
TCGGAAGAGCGTCGTGTAGG: 150
GAAGAGCGTCGTGTAGGGAA: 121
GAGCGTCGTGTAGGGAAAGA: 99
CGTCGTGTAGGGAAAGAGTG: 168
CGTGTAGGGAAAGAGTGTAG: 163
GTAGGGAAAGAGTGTAGATC: 199
GGGAAAGAGTGTAGATCTCG: 251
AAAGAGTGTAGATCTCGGTG: 241
GAGTGTAGATCTCGGTGGTC: 257
TGTAGATCTCGGTGGTCGCC: 266
AGATCTCGGTGGTCGCCGTA: 238
TCTCGGTGGTCGCCGTATCA: 205

bowtie2 -- Uh oh, did something wrong -- when my bowtie2 script gets done it ends up putting empty files back instead of the correctly filtered contents. Stupid! 
Luckily, my scripts still have it save the fastq files from right before it does the next index, so I can just recover those from the "pree_coli" fastqs. But I need to fix that
in the actual script -- I wonder what went wrong?

It isn't clear, but somehow the files got mv'd over each other in a way that erased them. 
*****Don't forget to copy the edited bowtie2_func.sh over to petulant-avenger and GIT COMMIT/PUSH!*******
 1106  qsub -o bw3.out -e bw3.err -N bw3 runbowtie.sh 3-ECcPro

 we ran bowtie this time requesting 8 cpus -- this does actually seem to go a bit faster. 
 There is not insignificant human genome contamination. 
 
 AFTER THIS, we run cutadapt. And then we're done boom. 
 THEN we do fastqc again, and hopefully we don't see any over-represented sequences that we'd attribute to anything but real transcript abundance...?
 
 Found what went wrong with bowtie. I had left out the name for the file for the filtered concordant reads to go to. Yup.
 
 ALSO, the index for e_coli didn't exist in the directory I told bowtie2 to look in. I've put it there now. 
 
 Just ran:
  1070  qsub -o bw3.out -e bw3.err -N bw3 runbowtie.sh 3-ECcPro

  The batch I should run for the rest of this set is:
  qsub -o bw10.out -e bw10.err -N bw10 runbowtie.sh 10-ECcWings
  
  
  OK we got this all working now
  cfisher@bbcsrv3 scratch]$ grep -c "+" *contams*
3-ECcProe_coli.P1.contams.fastq:205
3-ECcProe_coli.P2.contams.fastq:209
3-ECcProe_coli.U.contams.fastq:225
3-ECcProhg19.P1.contams.fastq:166409
3-ECcProhg19.P2.contams.fastq:168486
3-ECcProhg19.U.contams.fastq:62567

Need to adjust just a few things to clean up after ourselves -- removing things from scratch

qsub -o bw10.out -e 10-ECcWings.bowtie.log -N bw10 runbowtie.sh 10-ECcWings
qsub -o bw18.out -e 18-ECcLegs.bowtie.log -N bw18 runbowtie.sh 18-ECcLegs
qsub -o bw16out -e 16-ECcMeso.bowtie.log -N bw16 runbowtie.sh 16-ECcMeso

 
It looks like some combination of trimmomatic and bowtie2 have gotten rid of the contaminating adapters. Running getsubstrings on R1 reads with the PCR primer2 gave absolutely 0 hits. 
Before trimming and bowtie, there were at least 7,888 sequences with contamination, if not more. 
WHOOOOO!
OK so there's no need to run cutadapt and I don't really see a need to run SeqPrep either. I think we should just get all these kids together and start our 
trinity assembly for ECc!
I expect this to be the least complicated assembly because there were only 4 individuals used for this RNA pool. 

We were losing some reads that had been trimmed and filtered and were perfectly fine. 

 
[cfisher@bbcsrv3 QCPipeline]$ cp ../PEData/3-ECcPro_TTAGGC_L008_R1_001.fastq.gz input/3-ECcPro.R1.fastq.gz
[cfisher@bbcsrv3 QCPipeline]$ cp ../PEData/3-ECcPro_TTAGGC_L008_R2_001.fastq.gz input/3-ECcPro.R2.fastq.gz

Have to redo 3-ECcPro from beginning.

The other ones -- I've updated bowtie2_func.sh so that the unpaired reads that are filtered during QualTrimPE are named with something that ends we PE, so that they don't get over
written by qualtrimSE. 
